{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPamyd63GLZ7uSPS9bhujPF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avindumihisara0229-code/telco-churn-project/blob/main/telco_churn_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install keras-tuner imbalanced-learn"
      ],
      "metadata": {
        "id": "KXBJoe_MHVl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UwgvWE28HePH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Change this path if you saved it in a subfolder\n",
        "file_path = '/content/drive/MyDrive/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3bbGZNQsHnV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries for Task 1 (EDA)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "J-W1ho8gkeZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# --- IMPORTANT: Update this path if your file is in a folder ---\n",
        "file_path = '/content/drive/MyDrive/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 1. Look at the first few rows\n",
        "print(\"--- First 5 Rows ---\")\n",
        "display(df.head())\n",
        "\n",
        "# 2. Get information on columns and data types\n",
        "print(\"\\n--- Column Info (Data Types) ---\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "hI3_JgXKkgG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Drop the useless 'customerID' column\n",
        "df.drop('customerID', axis=1, inplace=True)\n",
        "\n",
        "# 2. Fix the 'TotalCharges' column\n",
        "# 'coerce' will turn any bad values (like empty strings) into 'NaT' (Not a Number)\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# 3. Check for any null values we just created\n",
        "print(\"--- Null Values After Cleaning ---\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "OUmGpt1pkklw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Drop the few rows with null values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# 5. Get statistics for our numerical columns\n",
        "print(\"\\n--- Numerical Statistics (After Clean) ---\")\n",
        "display(df.describe())"
      ],
      "metadata": {
        "id": "oOliF-4_km8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of our target variable 'Churn'\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Churn', data=df)\n",
        "plt.title('Churn Distribution (Target Variable)')\n",
        "plt.show()\n",
        "\n",
        "# exact percentages\n",
        "print(df['Churn'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "mOi2na20sX9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot numerical features\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Tenure vs. Churn\n",
        "sns.histplot(data=df, x='tenure', hue='Churn', multiple='stack', ax=axes[0])\n",
        "axes[0].set_title('Tenure vs. Churn')\n",
        "\n",
        "# MonthlyCharges vs. Churn\n",
        "sns.histplot(data=df, x='MonthlyCharges', hue='Churn', multiple='stack', ax=axes[1])\n",
        "axes[1].set_title('Monthly Charges vs. Churn')\n",
        "\n",
        "# TotalCharges vs. Churn\n",
        "sns.histplot(data=df, x='TotalCharges', hue='Churn', multiple='stack', ax=axes[2])\n",
        "axes[2].set_title('Total Charges vs. Churn')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "57N63lkb4CcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot key categorical features\n",
        "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
        "\n",
        "# Contract vs. Churn\n",
        "sns.countplot(data=df, x='Contract', hue='Churn', ax=axes[0])\n",
        "axes[0].set_title('Churn Rate by Contract Type')\n",
        "\n",
        "# Internet Service vs. Churn\n",
        "sns.countplot(data=df, x='InternetService', hue='Churn', ax=axes[1])\n",
        "axes[1].set_title('Churn Rate by Internet Service')\n",
        "\n",
        "# Payment Method vs. Churn\n",
        "sns.countplot(data=df, x='PaymentMethod', hue='Churn', ax=axes[2])\n",
        "axes[2].set_title('Churn Rate by Payment Method')\n",
        "plt.xticks(rotation=15) # Rotate labels for readability\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b9bLGKiU4DTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "1du7E2ly4Sem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports for Task 2 ---\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Imbalance Handling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score"
      ],
      "metadata": {
        "id": "eh_bIDws4FnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Encode the target variable 'Churn'\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})"
      ],
      "metadata": {
        "id": "BoU1QHHIC_Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define our features (X) and target (y)\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']"
      ],
      "metadata": {
        "id": "wbXvrEhnDNiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Identify numerical and categorical features\n",
        "# Numerical features\n",
        "numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "# Categorical features\n",
        "categorical_features = [col for col in X.columns if col not in numeric_features]\n",
        "\n",
        "print(f\"Numerical Features: {numeric_features}\")\n",
        "print(f\"Categorical Features: {categorical_features}\")"
      ],
      "metadata": {
        "id": "3GoAonmnDUD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,    # 20% for testing\n",
        "                                                    random_state=42,  # For reproducible results\n",
        "                                                    stratify=y)       # Keep class balance"
      ],
      "metadata": {
        "id": "WWrnKyG5DaIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Create the preprocessing transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore') # Ignores new categories in test set\n",
        "\n",
        "# 6. Create the preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])"
      ],
      "metadata": {
        "id": "EqA7o3R10rmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Apply the preprocessor to our training data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# 8. Apply the preprocessor to our testing data\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# 9. Handle Class Imbalance with SMOTE (on training data only)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "# Check the new shape and class distribution\n",
        "print(f\"Shape before SMOTE: {X_train_processed.shape}\")\n",
        "print(f\"Shape after SMOTE: {X_train_resampled.shape}\")\n",
        "print(f\"Original y_train distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Resampled y_train distribution:\\n{pd.Series(y_train_resampled).value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "id": "8GDLMhtC0tyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model 1: Decision Tree ---\n",
        "\n",
        "# 1. Create a pipeline that includes the preprocessor and the classifier\n",
        "# This ensures that preprocessing is correctly applied to each fold in GridSearchCV\n",
        "dt_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# 2. Define the parameter grid to search\n",
        "# These are the hyperparameters we want to tune\n",
        "param_grid_dt = {\n",
        "    'classifier__max_depth': [3, 5, 7, 10],\n",
        "    'classifier__min_samples_leaf': [5, 10, 20],\n",
        "    'classifier__criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# 3. Set up the GridSearch\n",
        "# We use 'roc_auc' as the scoring metric because the data is imbalanced\n",
        "# cv=5 means 5-fold cross-validation\n",
        "# n_jobs=-1 uses all available CPU cores to speed up training\n",
        "grid_search_dt = GridSearchCV(dt_pipeline, param_grid_dt, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "\n",
        "# 4. Fit the GridSearch to the *original* training data\n",
        "# The pipeline will handle preprocessing and\n",
        "# GridSearchCV will handle the model tuning.\n",
        "print(\"Starting Decision Tree Hyperparameter Tuning...\")\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "\n",
        "# 5. Get the best model\n",
        "best_dt = grid_search_dt.best_estimator_\n",
        "\n",
        "print(\"\\n--- Decision Tree Tuning Complete ---\")\n",
        "print(f\"Best DT Parameters: {grid_search_dt.best_params_}\")\n",
        "print(f\"Best DT Cross-Validation ROC-AUC Score: {grid_search_dt.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "w_rj0zxY0xW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Make predictions on the test set\n",
        "y_pred_dt = best_dt.predict(X_test)\n",
        "y_pred_proba_dt = best_dt.predict_proba(X_test)[:, 1] # Get probabilities for 'Yes' (class 1)\n",
        "\n",
        "# 2. Print Evaluation Metrics\n",
        "print(\"--- Decision Tree Test Set Evaluation ---\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_dt))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_dt, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "print(f\"\\nTest Set ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_dt):.4f}\")\n",
        "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")"
      ],
      "metadata": {
        "id": "SpjyWERd00r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get the input shape for the network's first layer\n",
        "# This is the number of features created by our preprocessor\n",
        "input_shape = X_train_resampled.shape[1]\n",
        "print(f\"Number of input features for NN: {input_shape}\")\n",
        "\n",
        "# 2. Define the model-building function for KerasTuner\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    hp_units_1 = hp.Int('units_1', min_value=16, max_value=64, step=16)\n",
        "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', [1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_units_1, activation='relu', input_shape=(input_shape,)))\n",
        "    model.add(Dropout(rate=hp_dropout))\n",
        "\n",
        "    # --- The first fix was here ---\n",
        "    model.add(Dense(units=hp_units_1 // 2, activation='relu'))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  # --- The second fix was here ---\n",
        "                  metrics=['auc']) # Make sure this is lowercase 'auc'\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "MP6w7dO309DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADD THIS NEW CELL ---\n",
        "# Delete the old tuner directory to clear the error\n",
        "!rm -rf ./my_dir\n",
        "\n",
        "print(\"Cleared the tuner's cache. Ready to try again.\")"
      ],
      "metadata": {
        "id": "sRAWFep51hZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set up the KerasTuner (we'll use RandomSearch)\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=kt.Objective(\"val_auc\", direction=\"max\"), # We want to maximize Validation AUC\n",
        "    max_trials=10,        # How many different models to try\n",
        "    executions_per_trial=2, # How many times to train each model\n",
        "    directory='my_dir',   # Folder to store results\n",
        "    project_name='churn_hpo'\n",
        ")\n",
        "\n",
        "# 2. Set up an EarlyStopping callback\n",
        "# This stops training if the model doesn't improve, saving time\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_auc',\n",
        "    mode='max',  # <--- This 'mode=max' is the key\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 3. Run the search\n",
        "# We use the *resampled* training data\n",
        "# We set validation_split=0.2 to hold out 20% of our *training* data\n",
        "# to validate the model during tuning.\n",
        "print(\"\\nStarting Neural Network Hyperparameter Tuning...\")\n",
        "tuner.search(X_train_resampled, y_train_resampled,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[early_stopping],\n",
        "             verbose=1)\n",
        "\n",
        "# 4. Get the best model\n",
        "best_nn_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "print(\"\\n--- Neural Network Tuning Complete ---\")\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "b0RlrtOj1iXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Make predictions on the test set\n",
        "# We get probabilities (a number between 0 and 1)\n",
        "y_pred_proba_nn = best_nn_model.predict(X_test_processed).ravel()\n",
        "\n",
        "# 2. Convert probabilities to class labels (0 or 1) using a 0.5 threshold\n",
        "y_pred_nn = (y_pred_proba_nn > 0.5).astype(int)\n",
        "\n",
        "# 3. Print Evaluation Metrics\n",
        "print(\"\\n--- Neural Network Test Set Evaluation ---\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_nn))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_nn, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "print(f\"\\nTest Set ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_nn):.4f}\")\n",
        "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred_nn):.4f}\")"
      ],
      "metadata": {
        "id": "DjYCAG5DESDA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}